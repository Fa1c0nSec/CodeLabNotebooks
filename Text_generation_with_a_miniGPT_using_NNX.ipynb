{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fa1c0nSec/CodeLabNotebooks/blob/master/Text_generation_with_a_miniGPT_using_NNX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvP1eNN_pExM"
      },
      "source": [
        "This is a direct translation of the [Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) tutorial from Keras to JAX. It aims to teach developers who are familiar with Keras/Tensorflow to pick up JAX/Flax quickly.\n",
        "\n",
        "This notebook demonstrates how to use [Flax NNX](https://flax.readthedocs.io/en/latest/nnx/index.html) to implement an autoregressive language model using a miniaturized version of the GPT model. The model uses only a single transformer block and is easy to understand.\n",
        "\n",
        "It is assumed that Colab T4 is used to run this notebook. Adjust the batch size if another hardware is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTmz5Cbco7n_"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zMsOIc7ouCO",
        "outputId": "37dd9e66-3567-483c-a3f2-31184fc898cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jax-ai-stack\n",
            "  Downloading jax_ai_stack-2024.11.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting jax==0.4.35 (from jax-ai-stack)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting flax==0.10.0 (from jax-ai-stack)\n",
            "  Downloading flax-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting ml-dtypes==0.4.0 (from jax-ai-stack)\n",
            "  Downloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting optax==0.2.3 (from jax-ai-stack)\n",
            "  Downloading optax-0.2.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting orbax-checkpoint==0.7.0 (from jax-ai-stack)\n",
            "  Downloading orbax_checkpoint-0.7.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting orbax-export==0.0.5 (from jax-ai-stack)\n",
            "  Downloading orbax_export-0.0.5-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax==0.10.0->jax-ai-stack) (1.1.0)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax==0.10.0->jax-ai-stack) (0.1.69)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax==0.10.0->jax-ai-stack) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax==0.10.0->jax-ai-stack) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax==0.10.0->jax-ai-stack) (6.0.2)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax==0.4.35->jax-ai-stack)\n",
            "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.35->jax-ai-stack) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.35->jax-ai-stack) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.35->jax-ai-stack) (1.13.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3->jax-ai-stack) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3->jax-ai-stack) (0.1.87)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3->jax-ai-stack) (1.11.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.7.0->jax-ai-stack) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.7.0->jax-ai-stack) (4.25.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.7.0->jax-ai-stack) (4.11.0)\n",
            "Collecting dataclasses-json (from orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting jaxtyping (from orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading jaxtyping-0.2.36-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax==0.2.3->jax-ai-stack) (0.12.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax==0.10.0->jax-ai-stack) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax==0.10.0->jax-ai-stack) (2.18.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.7.0->jax-ai-stack) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.7.0->jax-ai-stack) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.7.0->jax-ai-stack) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax==0.10.0->jax-ai-stack) (0.1.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->orbax-export==0.0.5->jax-ai-stack) (24.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading jax_ai_stack-2024.11.1-py3-none-any.whl (11 kB)\n",
            "Downloading flax-0.10.0-py3-none-any.whl (420 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.2/420.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optax-0.2.3-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orbax_checkpoint-0.7.0-py3-none-any.whl (279 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orbax_export-0.0.5-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl (87.3 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m83.9/87.3 MB\u001b[0m \u001b[31m158.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m      Successfully uninstalled jax-0.4.33\n",
            "  Attempting uninstall: orbax-checkpoint\n",
            "    Found existing installation: orbax-checkpoint 0.6.4\n",
            "    Uninstalling orbax-checkpoint-0.6.4:\n",
            "      Successfully uninstalled orbax-checkpoint-0.6.4\n",
            "  Attempting uninstall: optax\n",
            "    Found existing installation: optax 0.2.4\n",
            "    Uninstalling optax-0.2.4:\n",
            "      Successfully uninstalled optax-0.2.4\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.8.5\n",
            "    Uninstalling flax-0.8.5:\n",
            "      Successfully uninstalled flax-0.8.5\n",
            "Successfully installed dataclasses-json-0.6.7 flax-0.10.0 jax-0.4.35 jax-ai-stack-2024.11.1 jaxlib-0.4.35 jaxtyping-0.2.36 marshmallow-3.23.1 ml-dtypes-0.4.0 mypy-extensions-1.0.0 optax-0.2.3 orbax-checkpoint-0.7.0 orbax-export-0.0.5 typing-inspect-0.9.0\n",
            "Requirement already satisfied: jax[cuda12] in /usr/local/lib/python3.10/dist-packages (0.4.35)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.36,>=0.4.36 (from jax[cuda12])\n",
            "  Downloading jaxlib-0.4.36-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12]) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12]) (1.26.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.10/dist-packages (from jax[cuda12]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12]) (1.13.1)\n",
            "Collecting jax-cuda12-plugin<=0.4.36,>=0.4.36 (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading jax_cuda12_plugin-0.4.36-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax-cuda12-pjrt==0.4.36 (from jax-cuda12-plugin<=0.4.36,>=0.4.36->jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading jax_cuda12_pjrt-0.4.36-py3-none-manylinux2014_x86_64.whl.metadata (349 bytes)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12>=12.6.85 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.1 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (9.6.0.74)\n",
            "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (2.23.4)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (12.6.85)\n",
            "Downloading jaxlib-0.4.36-cp310-cp310-manylinux2014_x86_64.whl (100.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_plugin-0.4.36-cp310-cp310-manylinux2014_x86_64.whl (16.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_pjrt-0.4.36-py3-none-manylinux2014_x86_64.whl (101.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.36-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jax-cuda12-pjrt, jax-cuda12-plugin, jaxlib, jax\n",
            "  Attempting uninstall: jax-cuda12-pjrt\n",
            "    Found existing installation: jax-cuda12-pjrt 0.4.33\n",
            "    Uninstalling jax-cuda12-pjrt-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-pjrt-0.4.33\n",
            "  Attempting uninstall: jax-cuda12-plugin\n",
            "    Found existing installation: jax-cuda12-plugin 0.4.33\n",
            "    Uninstalling jax-cuda12-plugin-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-plugin-0.4.33\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.35\n",
            "    Uninstalling jaxlib-0.4.35:\n",
            "      Successfully uninstalled jaxlib-0.4.35\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.35\n",
            "    Uninstalling jax-0.4.35:\n",
            "      Successfully uninstalled jax-0.4.35\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax-ai-stack 2024.11.1 requires jax==0.4.35, but you have jax 0.4.36 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.4.36 jax-cuda12-pjrt-0.4.36 jax-cuda12-plugin-0.4.36 jaxlib-0.4.36\n"
          ]
        }
      ],
      "source": [
        "!pip install jax-ai-stack\n",
        "!pip install -U \"jax[cuda12]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHzJ_bokoovZ"
      },
      "source": [
        "Grab the IMDB review data as the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olwq3MrpojcJ",
        "outputId": "4c6060a4-ded5-4a15-b09d-83245d23461f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  41.0M      0  0:00:01  0:00:01 --:--:-- 41.0M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKE2uUafLobI"
      },
      "source": [
        "Take care of the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MKYFNOhdLq98"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax\n",
        "from typing import Any\n",
        "import os\n",
        "import string\n",
        "import random\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Stall8nIGV8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyt7MV6prz1"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Next, define the model architecture, which is a decoder-only transformer model. The model is similar to the GPT model series but it's smaller in size with only one transformer block, which is why we are calling it miniGPT. The model has several key components stacked up together, so let's go over the them one by one.\n",
        "\n",
        "The key component is the `TransformerBlock`, which uses the multi-head attention mechanism as described in the famous [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper. Please get familiar with the paper if you are not already because we are going to implement some of the details below.\n",
        "\n",
        "The model is auto-regressive, so it can only attend to previous tokens. So we use [`jax.numpy.tril`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tril.html) to create the attention mask, and pass it in the `nnx.MultiHeadAttention` layer. The other layers follow the practice of the decoder layer in the paper.\n",
        "\n",
        "All layers (except `Dropout`) has a `rngs` parameter, which is the [random generator key](https://jax.readthedocs.io/en/latest/jax.random.html#prng-keys) that can help you reproduce results and debug issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z0p-IHurrB9i"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads, in_features=embed_dim, rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6, num_features=embed_dim, rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim, out_features=ff_dim, rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim, out_features=embed_dim, rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6, num_features=embed_dim, rngs=rngs)\n",
        "\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        batch_size, seq_len, _ = input_shape\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = causal_attention_mask(seq_len)\n",
        "\n",
        "        # Apply MultiHeadAttention with causal mask\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=inputs,\n",
        "            mask=mask,\n",
        "            decode=False\n",
        "        )\n",
        "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
        "        out1 = self.layer_norm1(inputs + attention_output)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.linear1(out1)\n",
        "        ffn_output = nnx.relu(ffn_output)\n",
        "        ffn_output = self.linear2(ffn_output)\n",
        "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
        "\n",
        "        return self.layer_norm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVA647SA8mQT"
      },
      "source": [
        "Since the model input is just text tokens, we need to convert them into embeddings. We use two kinds of embeddings: token embedding and position embeddings, both of which are learned by the model and are added up. Note that this is slightly different from the origianl paper, which uses static, instead of learned, positional embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ywxWh4cg5Kh2"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return token_embedding + position_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUTg9IxJ8-Q1"
      },
      "source": [
        "Now we can put everything together to build our miniGPT model. We convert the tokens into embeddings, add a single `TransformerBlock` and finally use a linear projection layer for output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YmUaAvr75SvU"
      },
      "outputs": [],
      "source": [
        "class MiniGPT(nnx.Module):\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
        "        )\n",
        "        self.output_layer = nnx.Linear(in_features=embed_dim, out_features=vocab_size, rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.transformer_block(x, training=training)\n",
        "        outputs = self.output_layer(x)\n",
        "        return outputs\n",
        "\n",
        "def create_model(rngs):\n",
        "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, rngs=rngs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igX_eoGNMTGR"
      },
      "source": [
        "Set some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GRhiDsCrMZRp"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "maxlen = 80\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "feed_forward_dim = 256\n",
        "batch_size = 512 # for Colab T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI1ci-HyMspJ"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "Data loading and preprocessing. To map the words and symbols to indices, we need to tokenize them first. For simplicity, we are using a vey simple tokenization scheme:\n",
        "* The `custom_standardization` function does some preprocessing by removing undesirable symbols and adding space before punctuations, so that punctuations can be treated as tokens like words\n",
        "* The `build_vocab` function builds our own vocaulary according to the `vocab_size` defined above\n",
        "* The `tokenize` function does the tokenization\n",
        "* We also batch the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGUFsn1GMuzh",
        "outputId": "d48eba20-67d0-4067-8836-e11da471690d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000 files\n"
          ]
        }
      ],
      "source": [
        "# Data loading and preprocessing\n",
        "filenames = []\n",
        "directories = [\n",
        "    \"./aclImdb/train/pos\",\n",
        "    \"./aclImdb/train/neg\",\n",
        "    \"./aclImdb/test/pos\",\n",
        "    \"./aclImdb/test/neg\",\n",
        "]\n",
        "for dir in directories:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "print(f\"{len(filenames)} files\")\n",
        "\n",
        "random.shuffle(filenames)\n",
        "\n",
        "# Custom text processing: add space before and after punctuations for tokenization\n",
        "def custom_standardization(input_string):\n",
        "    lowercased = input_string.lower()\n",
        "    stripped_html = lowercased.replace(\"<br />\", \" \")\n",
        "    return ''.join([' ' + char + ' ' if char in string.punctuation else char for char in stripped_html]).strip()\n",
        "\n",
        "def build_vocab(texts, vocab_size):\n",
        "    all_words = ' '.join(texts).split()\n",
        "    word_counts = Counter(all_words)\n",
        "    vocab = ['<PAD>', '<UNK>'] + [word for word, _ in word_counts.most_common(vocab_size - 2)]\n",
        "    word_to_index = {word: index for index, word in enumerate(vocab)}\n",
        "    return vocab, word_to_index\n",
        "\n",
        "def tokenize(text, word_to_index, maxlen):\n",
        "    words = text.split()\n",
        "    tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in words]\n",
        "    if len(tokens) < maxlen:\n",
        "        tokens = tokens + [word_to_index['<PAD>']] * (maxlen - len(tokens))\n",
        "    else:\n",
        "        tokens = tokens[:maxlen]\n",
        "    return tokens\n",
        "\n",
        "def load_and_preprocess_data(filenames, batch_size, vocab_size, maxlen):\n",
        "    data = []\n",
        "    for filename in filenames:\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            processed_text = custom_standardization(text)\n",
        "            data.append(processed_text)\n",
        "\n",
        "    vocab, word_to_index = build_vocab(data, vocab_size)\n",
        "    tokenized_data = [tokenize(text, word_to_index, maxlen) for text in data]\n",
        "\n",
        "    # Batch the data\n",
        "    batched_data = [tokenized_data[i:i+batch_size] for i in range(0, len(tokenized_data), batch_size)]\n",
        "\n",
        "    return batched_data, vocab, word_to_index\n",
        "\n",
        "text_ds, vocab, word_to_index = load_and_preprocess_data(filenames, batch_size, vocab_size, maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKVSD8KSM1um"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Define a helper function for generating text given a model and prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_f4rEMm4M5lg"
      },
      "outputs": [],
      "source": [
        "def generate_text(model: MiniGPT, max_tokens: int, start_tokens: [int], index_to_word: [str], top_k=10):\n",
        "    def sample_from(logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    def generate_step(start_tokens):\n",
        "        pad_len = maxlen - len(start_tokens)\n",
        "        sample_index = len(start_tokens) - 1\n",
        "        if pad_len < 0:\n",
        "            x = jnp.array(start_tokens[:maxlen])\n",
        "            sample_index = maxlen - 1\n",
        "        elif pad_len > 0:\n",
        "            x = jnp.array(start_tokens + [0] * pad_len)\n",
        "        else:\n",
        "            x = jnp.array(start_tokens)\n",
        "\n",
        "        x = x[None, :]\n",
        "        logits = model(x)\n",
        "        next_token = sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    generated = []\n",
        "    for _ in range(max_tokens):\n",
        "        next_token = generate_step(start_tokens + generated)\n",
        "        generated.append(int(next_token))\n",
        "    return \" \".join([index_to_word[token] for token in start_tokens + generated])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkuaFXkANFNp"
      },
      "source": [
        "Define the loss function and training step function. The `train_step` is usually the most expensive function since it needs to compute the gradients and update the model parameters. We can use [JAX JIT compilation](https://jax.readthedocs.io/en/latest/jit-compilation.html#jit-compiling-a-function) to accelerate the execution of this function, but since we using NNX here, we annoate it with `@nnx.jit` instead of `@jax.jit`. JIT-compiled functions sometimes are tricky to debug; please refer to our [debugging documentation](https://jax.readthedocs.io/en/latest/debugging/print_breakpoint.html#compiled-prints-and-breakpoints) for help if you encouter such a situation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8rRuTmABNV4b"
      },
      "outputs": [],
      "source": [
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5um2vkeUNckm"
      },
      "source": [
        "Start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysl6CsfENeJN",
        "outputId": "7e527fef-ceca-479b-d047-a21d458a2dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial generated text:\n",
            "this movie is seem unsympathetic capturing clarity vermont detonator plant chagrin snatch linden dullness reginald futility cambodian derived inside polarisdib renewed dingo intrusive thrillers seminal shrewd ff pianiste filled basil harmless homoerotic alibi india sasquatch talkative departments ramblings english admirably phallic landscape jeans\n",
            "\n",
            "Epoch 1, Loss: 6.118307113647461\n",
            "Generated text:\n",
            "this movie is one of the story . . i have ever seen the first film was so many movies , and it is one of the <UNK> and it ' t be the <UNK> of the <UNK> of the story . it\n",
            "\n",
            "Epoch 2, Loss: 5.04942512512207\n",
            "Generated text:\n",
            "this movie is not one of the best movie , but i can be . the best movie i can be seen in this is the film is one of the first saw the worst film is one of the movie , i\n",
            "\n",
            "Epoch 3, Loss: 4.757480144500732\n",
            "Generated text:\n",
            "this movie is one of the worst films ever made in my mind , and i was very impressed with this film . the story was not very bad , the movie was so much , but the first i have to watch\n",
            "\n",
            "Epoch 4, Loss: 4.550637245178223\n",
            "Generated text:\n",
            "this movie is one of the worst movie i ' ve ever seen . it ' s not only the first , i can ' t say i have to admit , but it is the first , it is the worst film\n",
            "\n",
            "Epoch 5, Loss: 4.408133506774902\n",
            "Generated text:\n",
            "this movie is one of the worst films i have ever seen . it is the best , and the worst film i ' ve ever seen in my opinion , but it ' s not even worse than a good horror movie\n",
            "\n",
            "Epoch 6, Loss: 4.296973705291748\n",
            "Generated text:\n",
            "this movie is about the people who lives of love with his <UNK> . it ' s not to mention a very bad film , it is so funny and funny , and entertaining . it is the story is about the same\n",
            "\n",
            "Epoch 7, Loss: 4.2042365074157715\n",
            "Generated text:\n",
            "this movie is about the same time and i think i was very impressed with this movie . it ' s the acting is very bad , but it ' s a shame it is so hard to find a good story ,\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "optimizer = nnx.Optimizer(model, optax.adam(1e-3))\n",
        "metrics = nnx.MultiMetric(\n",
        "  loss=nnx.metrics.Average('loss'),\n",
        "  # You can add additional metrics for tracking\n",
        ")\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"this movie is\"\n",
        "start_tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in start_prompt.split()]\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n",
        "generated_text = generate_text(\n",
        "    model, 40, start_tokens, index_to_word\n",
        ")\n",
        "print(f\"Initial generated text:\\n{generated_text}\\n\")\n",
        "\n",
        "num_epochs = 25\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in text_ds:\n",
        "        input_batch = jnp.array(batch)\n",
        "        target_batch = jnp.array([tokens[1:] + [word_to_index['<PAD>']] for tokens in batch])\n",
        "        train_step(model, optimizer, metrics, (input_batch, target_batch))\n",
        "\n",
        "    for metric, value in metrics.compute().items():  # compute metrics\n",
        "      metrics_history[f'train_{metric}'].append(value)  # record metrics\n",
        "    metrics.reset()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {metrics_history['train_loss'][-1]}\")\n",
        "    start_prompt = \"this movie is\"\n",
        "    start_tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in start_prompt.split()]\n",
        "    generated_text = generate_text(\n",
        "        model, 40, start_tokens, index_to_word\n",
        "    )\n",
        "    print(f\"Generated text:\\n{generated_text}\\n\")\n",
        "\n",
        "# Final text generation\n",
        "start_tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in start_prompt.split()]\n",
        "generated_text = generate_text(\n",
        "    model, 40, start_tokens, index_to_word\n",
        ")\n",
        "print(f\"Final generated text:\\n{generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB-ExEt1Zl1C"
      },
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sentences that look like sensible movie reviews at the end of the training. Of course the reviews are far from perfect because this model is really small and fundamentally lacks strong intelligence like modern LLMs. In our next tutorial, we are going to scale the model up and make it smarter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pKD3c1UMu7x"
      },
      "source": [
        "## Save the model\n",
        "\n",
        "We use [Orbax](https://github.com/google/orbax) to save the model checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkoFGCgSZ1yz",
        "outputId": "cb85e0b6-4ba3-44d9-d576-8160920d0c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_CHECKPOINT_METADATA  d  manifest.ocdbt  _METADATA  ocdbt.process_0  _sharding\n"
          ]
        }
      ],
      "source": [
        "import orbax.checkpoint as orbax\n",
        "\n",
        "state = nnx.state(model)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "checkpointer.save('/content/save', state)\n",
        "\n",
        "# Make sure the files are there\n",
        "!ls /content/save/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zrue6HWMwkG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}